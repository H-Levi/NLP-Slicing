{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7856682",
   "metadata": {
    "id": "d7856682"
   },
   "source": [
    "### Section 1: Library Installations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KbG11Z9sKHP3",
   "metadata": {
    "id": "KbG11Z9sKHP3"
   },
   "source": [
    "> Install necessary libraries for Wikipedia scraping, natural language processing, and Lambda LLM model interaction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "316542f0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "316542f0",
    "outputId": "6dfd510b-00c7-47a1-f11c-d1fd71aee71b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wikipedia-api in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (0.6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from wikipedia-api) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from requests->wikipedia-api) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\creating_an_enviroment\\lib\\site-packages (from requests->wikipedia-api) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from requests->wikipedia-api) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from requests->wikipedia-api) (2023.11.17)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wikipedia in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\envs\\creating_an_enviroment\\lib\\site-packages (from wikipedia) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from wikipedia) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\creating_an_enviroment\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.11.17)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\envs\\creating_an_enviroment\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.5)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\creating_an_enviroment\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (1.4.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn) (3.2.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting replicate\n",
      "  Downloading replicate-0.23.1-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting httpx<1,>=0.21.0 (from replicate)\n",
      "  Downloading httpx-0.26.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\envs\\creating_an_enviroment\\lib\\site-packages (from replicate) (23.1)\n",
      "Collecting pydantic>1 (from replicate)\n",
      "  Downloading pydantic-2.6.0-py3-none-any.whl.metadata (81 kB)\n",
      "     ---------------------------------------- 0.0/81.8 kB ? eta -:--:--\n",
      "     ----- ---------------------------------- 10.2/81.8 kB ? eta -:--:--\n",
      "     ------------------------------ --------- 61.4/81.8 kB 1.1 MB/s eta 0:00:01\n",
      "     --------------------------------- ---- 71.7/81.8 kB 991.0 kB/s eta 0:00:01\n",
      "     --------------------------------- ---- 71.7/81.8 kB 991.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- 81.8/81.8 kB 458.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from replicate) (4.9.0)\n",
      "Requirement already satisfied: anyio in c:\\programdata\\anaconda3\\envs\\creating_an_enviroment\\lib\\site-packages (from httpx<1,>=0.21.0->replicate) (3.5.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from httpx<1,>=0.21.0->replicate) (2023.11.17)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.21.0->replicate)\n",
      "  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\envs\\creating_an_enviroment\\lib\\site-packages (from httpx<1,>=0.21.0->replicate) (3.4)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\envs\\creating_an_enviroment\\lib\\site-packages (from httpx<1,>=0.21.0->replicate) (1.2.0)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.21.0->replicate)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
      "     --------------------- ------------------ 30.7/58.3 kB 1.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 51.2/58.3 kB 1.3 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 51.2/58.3 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 58.3/58.3 kB 384.2 kB/s eta 0:00:00\n",
      "Collecting annotated-types>=0.4.0 (from pydantic>1->replicate)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.16.1 (from pydantic>1->replicate)\n",
      "  Downloading pydantic_core-2.16.1-cp311-none-win_amd64.whl.metadata (6.6 kB)\n",
      "Downloading replicate-0.23.1-py3-none-any.whl (36 kB)\n",
      "Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
      "   ---------------------------------------- 0.0/75.9 kB ? eta -:--:--\n",
      "   -------------------------------- ------- 61.4/75.9 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 75.9/75.9 kB 841.3 kB/s eta 0:00:00\n",
      "Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/76.9 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 41.0/76.9 kB 991.0 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 71.7/76.9 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 76.9/76.9 kB 855.2 kB/s eta 0:00:00\n",
      "Downloading pydantic-2.6.0-py3-none-any.whl (394 kB)\n",
      "   ---------------------------------------- 0.0/394.2 kB ? eta -:--:--\n",
      "   ---- ---------------------------------- 41.0/394.2 kB 991.0 kB/s eta 0:00:01\n",
      "   --------- ------------------------------ 92.2/394.2 kB 1.1 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 143.4/394.2 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------- --------------------- 174.1/394.2 kB 952.6 kB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 225.3/394.2 kB 1.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 266.2/394.2 kB 1.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 317.4/394.2 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 368.6/394.2 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 394.2/394.2 kB 1.0 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.16.1-cp311-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.9 MB 1.9 MB/s eta 0:00:01\n",
      "   - -------------------------------------- 0.1/1.9 MB 1.3 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.1/1.9 MB 1.2 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.2/1.9 MB 1.2 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.3/1.9 MB 1.2 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.3/1.9 MB 1.2 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.4/1.9 MB 1.2 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.5/1.9 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 0.5/1.9 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 0.6/1.9 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.6/1.9 MB 1.2 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 0.7/1.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.8/1.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.8/1.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.9/1.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.9/1.9 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.0/1.9 MB 1.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.0/1.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.1/1.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.2/1.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.2/1.9 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.3/1.9 MB 1.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.3/1.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.4/1.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.4/1.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.5/1.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.5/1.9 MB 1.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.6/1.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.6/1.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.7/1.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.8/1.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.8/1.9 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.9/1.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/1.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/1.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/1.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 1.1 MB/s eta 0:00:00\n",
      "Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: pydantic-core, h11, annotated-types, pydantic, httpcore, httpx, replicate\n",
      "Successfully installed annotated-types-0.6.0 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 pydantic-2.6.0 pydantic-core-2.16.1 replicate-0.23.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script httpx.exe is installed in 'C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia-api\n",
    "!pip install wikipedia\n",
    "!pip install nltk\n",
    "!pip install scikit-learn\n",
    "!pip install replicate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JjryA75JJ2T1",
   "metadata": {
    "id": "JjryA75JJ2T1"
   },
   "source": [
    "### Section 2: Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04eedd7c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "04eedd7c",
    "outputId": "87ceb998-b2ff-4447-b5ac-5007ab0e0352"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import wikipedia\n",
    "import wikipediaapi\n",
    "import re\n",
    "import os\n",
    "import replicate\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6LhNYmnZK8K4",
   "metadata": {
    "id": "6LhNYmnZK8K4"
   },
   "source": [
    "### Section 3: Fetching Wikipedia Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EPl9knfGLA5a",
   "metadata": {
    "id": "EPl9knfGLA5a"
   },
   "source": [
    "> Fetch Wikipedia content related to the specified title, 'medical', using the Wikipedia API, store it in a dictionary, and save it to a JSON file named 'medical_data_2MB.json', limiting the dataset size to 2 MB.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cc2a837",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "4cc2a837",
    "outputId": "4ad5e161-8886-4e60-8602-da5bc06a2aeb"
   },
   "outputs": [],
   "source": [
    "# Define the topic of interest\n",
    "medical_title = 'Medical Science'\n",
    "\n",
    "# Assign a user_agent for Wikipedia API requests\n",
    "user_agent = 'hailegabriel/1.0 (bentitan98@gmail.com)'\n",
    "wiki_wiki = wikipediaapi.Wikipedia(user_agent, 'en', wikipediaapi.ExtractFormat.WIKI)\n",
    "\n",
    "# Initialize variables for data storage and size control\n",
    "my_file = {}\n",
    "total_size_mb = 0\n",
    "i = 0\n",
    "\n",
    "# Fetch Wikipedia content related to the medical_title\n",
    "for items in wikipedia.search(medical_title, results=500):  # Adjust result count as needed\n",
    "    # Retrieve page content\n",
    "    page_content = wiki_wiki.page(items).text\n",
    "\n",
    "    # Calculate content size in megabytes\n",
    "    content_size_mb = len(page_content.encode('utf-8')) / (1024 ** 2)\n",
    "\n",
    "    # Limit the dataset size to 2 MB\n",
    "    if total_size_mb + content_size_mb > 2:\n",
    "        break\n",
    "\n",
    "    # Store page content in dictionary\n",
    "    my_file[items] = page_content\n",
    "\n",
    "    # Update total size\n",
    "    total_size_mb += content_size_mb\n",
    "\n",
    "# Create a JSON file and write the content\n",
    "with open('medical_data_2MB.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(my_file, json_file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3XGxlijsMPO_",
   "metadata": {
    "id": "3XGxlijsMPO_"
   },
   "source": [
    "### Section 4: Text Cleaning and Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RVstIziBMXqT",
   "metadata": {
    "id": "RVstIziBMXqT"
   },
   "source": [
    "> Create text cleaning functions to eliminate unnecessary elements like newline characters, reference-style links, extra whitespaces, backslashes, and double-quotes. Also, implement a function to clean the data, save the cleaned content in a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f492248e",
   "metadata": {
    "id": "f492248e"
   },
   "outputs": [],
   "source": [
    "def clean_wikipedia_text(wiki_text):\n",
    "    # Remove lines with \"\\n\" and lines containing '=='\n",
    "    cleaned_text = re.sub(r'\\n|==.*?==', ' ', wiki_text)\n",
    "\n",
    "    # Remove any remaining reference-style links (e.g., [1], [2])\n",
    "    cleaned_text = re.sub(r'\\[\\d+\\]', '', cleaned_text)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "\n",
    "    # Remove backslashes and double-quotes\n",
    "    cleaned_text = cleaned_text.replace('\\\\', '').replace('\"', '')\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def clean_and_convert_to_string(input_file):\n",
    "    # Read the JSON file\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Clean each value in the JSON using clean_wikipedia_text function\n",
    "    cleaned_data = {key: clean_wikipedia_text(value) for key, value in data.items()}\n",
    "\n",
    "    # Convert the cleaned data to a single string\n",
    "    cleaned_text = ' '.join(cleaned_data.values())\n",
    "\n",
    "    # Specify the path using double backslashes or a raw string\n",
    "    cleaned_medical_data_path = 'cleaned_medical_data.txt'  # or r'C:\\Users\\Admin\\Documents\\nlp\\cleaned_medical_data.txt'\n",
    "\n",
    "    # Write cleaned text to a new file\n",
    "    with open(cleaned_medical_data_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(cleaned_text)\n",
    "\n",
    "    return cleaned_medical_data_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Jdd1HjXTNnWj",
   "metadata": {
    "id": "Jdd1HjXTNnWj"
   },
   "source": [
    "\n",
    "\n",
    "> Read the newly obtained txt file containing cleaned medical text data and store its content in a  variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd04f991",
   "metadata": {
    "id": "cd04f991"
   },
   "outputs": [],
   "source": [
    "# Use the wikipedia medical json file\n",
    "\n",
    "input_json_file = r'medical_data_2MB.json'\n",
    "cleaned_text_path = clean_and_convert_to_string(input_json_file)\n",
    "\n",
    "# Read the cleaned text file\n",
    "with open(cleaned_text_path, 'r', encoding='utf-8') as file:\n",
    "    input_text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0oeC0NNXO_tD",
   "metadata": {
    "id": "0oeC0NNXO_tD"
   },
   "source": [
    "### Section 5: Preprocessing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U7D2k4VMO8ew",
   "metadata": {
    "id": "U7D2k4VMO8ew"
   },
   "source": [
    "> Preprocess text by removing special characters, preserving hashtags and mentions, tokenizing using NLTK, converting to lowercase, removing stopwords, and lemmatizing using WordNetLemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b858c5b",
   "metadata": {
    "id": "5b858c5b"
   },
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Special Characters Removal: Preserve hashtags and mentions\n",
    "    clean_text = re.sub(r\"(?<=\\w)[^\\w\\s]+(?=\\w)|[^\\w\\s]\", ' ', text)\n",
    "\n",
    "    # Tokenization: Use nltk.word_tokenize or explore other methods\n",
    "    tokens = nltk.word_tokenize(clean_text)\n",
    "\n",
    "    # Lowercasing: Convert text to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # Stopword Removal: Customize the list of stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatization: Use WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eZj_NqUcPRSE",
   "metadata": {
    "id": "eZj_NqUcPRSE"
   },
   "source": [
    "### Section 6: Cosine Similarity Calculation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kH52VHL0PoJo",
   "metadata": {
    "id": "kH52VHL0PoJo"
   },
   "source": [
    "\n",
    "> Check the cosine similarity between two text slices using TF-IDF vectorization. The function returns True if the similarity is equal to or greater than a specified threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "354e7035",
   "metadata": {
    "id": "354e7035"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity_checker(slice1, slice2, threshold=0.7):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([slice1, slice2])\n",
    "    similarity = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
    "    return similarity >= threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0PzwsgJqPxsZ",
   "metadata": {
    "id": "0PzwsgJqPxsZ"
   },
   "source": [
    "### Section 7: Text Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8QlWtuAFQ1Uq",
   "metadata": {
    "id": "8QlWtuAFQ1Uq"
   },
   "source": [
    "\n",
    "> Generate text slices from the input text, adhering to a size limit in kilobytes. The function tokenizes the input into sentences, creating slices while ensuring each slice's size doesn't surpass the specified limit and is dissimilar enough to the previous slices. The similarity check is performed using cosine similarity with the cosine_similarity_checker function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca3abaa3",
   "metadata": {
    "id": "ca3abaa3"
   },
   "outputs": [],
   "source": [
    "def generate_slices_by_size(input_text, size_limit=20):\n",
    "    # Convert size_limit to bytes\n",
    "    size_limit_bytes = size_limit * 1024\n",
    "\n",
    "    # Tokenize the input text\n",
    "    sentences = sent_tokenize(input_text)\n",
    "\n",
    "    # Generate slices\n",
    "    slices = []\n",
    "    current_slice = b\"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Encode the current token in utf-8\n",
    "        sentence_bytes = sentence.encode('utf-8')\n",
    "\n",
    "        # Check if adding the next token exceeds the size limit\n",
    "        if len(current_slice) + len(sentence_bytes) + 1 <= size_limit_bytes:\n",
    "            current_slice += b\" \" + sentence_bytes\n",
    "        else:\n",
    "            # Check for duplicate before adding the new slice\n",
    "            current_preprocessed_slice = preprocess_text(current_slice.decode('utf-8'))\n",
    "\n",
    "            # Preprocess each previous slice before comparison\n",
    "            if not any(cosine_similarity_checker(current_preprocessed_slice, preprocess_text(previous_slice.decode('utf-8'))) for previous_slice in slices):\n",
    "                slices.append(current_slice)\n",
    "\n",
    "            # Start a new slice with the current token\n",
    "            current_slice = b\" \" + sentence_bytes\n",
    "\n",
    "    # Check for the last slice after the loop\n",
    "    if current_slice:\n",
    "        current_preprocessed_slice = preprocess_text(current_slice.decode('utf-8'))\n",
    "        # Preprocess each previous slice before comparison\n",
    "        if not any(cosine_similarity_checker(current_preprocessed_slice, preprocess_text(previous_slice.decode('utf-8'))) for previous_slice in slices):\n",
    "            slices.append(current_slice)\n",
    "\n",
    "    return slices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VKll4w4QRA2h",
   "metadata": {
    "id": "VKll4w4QRA2h"
   },
   "source": [
    "> Generate text slices from the provided input text and save them to a text file. Each slice is written on a new line in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "lm8xcRc75CiA",
   "metadata": {
    "id": "lm8xcRc75CiA"
   },
   "outputs": [],
   "source": [
    "slices = generate_slices_by_size(input_text)\n",
    "\n",
    "# Save slices to a txt file\n",
    "with open('slices.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for i, slice in enumerate(slices):\n",
    "        output_file.write(f\"{slice}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Dg-eKorfRBIT",
   "metadata": {
    "id": "Dg-eKorfRBIT"
   },
   "source": [
    "### Section 8: Regeneration and Similarity Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LZQwz1LxSFBc",
   "metadata": {
    "id": "LZQwz1LxSFBc"
   },
   "source": [
    "\n",
    "\n",
    "> The code effectively reconstructs the original text from the provided text slices and saves it to a file. To ensure the quality of regeneration, the script evaluates the similarity between the content of the original and regenerated files,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "NwUrDUHyG93a",
   "metadata": {
    "id": "NwUrDUHyG93a"
   },
   "outputs": [],
   "source": [
    "def regenerate_sliced_file(slices, output_file):\n",
    "    # Join the list of slices to reconstruct the original text\n",
    "    reconstructed_text = b\"\".join(slices).decode('utf-8')\n",
    "\n",
    "    # Write the reconstructed text to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(reconstructed_text)\n",
    "\n",
    "# Specify the output file path\n",
    "regenerated_file_path = r'regenerated_medical_data.txt'  # or r'C:\\Users\\Admin\\Documents\\nlp\\regenerated_medical_data.txt'\n",
    "\n",
    "# Call the function to regenerate the sliced file\n",
    "regenerate_sliced_file(slices, regenerated_file_path)\n",
    "\n",
    "# Check if the regeneration is successful by printing the content of the regenerated file\n",
    "with open(regenerated_file_path, 'r', encoding='utf-8') as file:\n",
    "    regenerated_text = file.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xa3iPbqQR_tS",
   "metadata": {
    "id": "xa3iPbqQR_tS"
   },
   "source": [
    "> This code calculates the cosine similarity between the original and regenerated texts after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bSIURF_DHZ-L",
   "metadata": {
    "id": "bSIURF_DHZ-L"
   },
   "outputs": [],
   "source": [
    "def calculate_similarity(original_text, regenerated_text):\n",
    "    # Preprocess the texts\n",
    "    preprocessed_original = preprocess_text(original_text)\n",
    "    preprocessed_regenerated = preprocess_text(regenerated_text)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([preprocessed_original, preprocessed_regenerated])\n",
    "    similarity = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
    "\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3XohMD3zTe4N",
   "metadata": {
    "id": "3XohMD3zTe4N"
   },
   "source": [
    "> This code calculates the similarity score between the original and regenerated text, providing insights into the effectiveness of the regeneration process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adqRHaaUHXiV",
   "metadata": {
    "id": "adqRHaaUHXiV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 0.9866304669360232\n",
      "Original Text Size (Bytes): 2088620\n",
      "Regenerated Text Size (Bytes): 1619998\n",
      "Original Text Word Count: 352710\n",
      "Regenerated Text Word Count: 272120\n"
     ]
    }
   ],
   "source": [
    "# Calculate similarity between original and regenerated text\n",
    "similarity_score = calculate_similarity(input_text, regenerated_text)\n",
    "\n",
    "# Print the results\n",
    "print(\"Similarity Score:\", similarity_score)\n",
    "print(\"Original Text Size (Bytes):\", len(input_text.encode('utf-8')))\n",
    "print(\"Regenerated Text Size (Bytes):\", len(regenerated_text.encode('utf-8')))\n",
    "print(\"Original Text Word Count:\", len(word_tokenize(input_text)))\n",
    "print(\"Regenerated Text Word Count:\", len(word_tokenize(regenerated_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48STDEtfTxkG",
   "metadata": {
    "id": "48STDEtfTxkG"
   },
   "source": [
    "### Section 8: Model Interaction with Replicate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3XEhdWkMUJl7",
   "metadata": {
    "id": "3XEhdWkMUJl7"
   },
   "source": [
    "> This code demonstrates the interaction with the OpenAI model. It loads text slices from a file, and then engages with the LLM model called LLAMA, by providing initial input and subsequently asking questions based on user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddda41ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: what is medicine?\n",
      "\n",
      "\n",
      "Hello! I'm happy to help you with your question.\n",
      "\n",
      "Medicine is a broad term that refers to the science and practice of preventing, diagnosing, and treating diseases or medical conditions. It encompasses various fields such as pharmacology, surgery, psychiatry, and epidemiology, among others. The primary goal of medicine is to promote health, well-being, and quality of life for individuals and communities.\n",
      "\n",
      "There are many different types of medicine, including:\n",
      "\n",
      "1. Western medicine: Also known as allopathic medicine, this"
     ]
    }
   ],
   "source": [
    "import replicate\n",
    "\n",
    "# Your API token\n",
    "REPLICATE_API_TOKEN = \"r8_4LS67CTNWRMuDy6XVykW6ooXMlEklDM1mGR5X\"\n",
    "\n",
    "# Initialize the Replicate client with your API token\n",
    "client = replicate.Client(api_token=REPLICATE_API_TOKEN)\n",
    "\n",
    "# Define your model\n",
    "model_name = \"meta/llama-2-70b-chat\"\n",
    "\n",
    "# Read slice text from input.txt\n",
    "with open('slices.txt', 'r', encoding='utf-8') as input_file:\n",
    "    slice_text = input_file.read()\n",
    "\n",
    "# Provide initial input to the model\n",
    "for event in client.stream(\n",
    "    model_name,\n",
    "    input={\n",
    "        \"prompt\": f\"Initial Input:\\n\\n{slice_text}\\n\\nUser Input:\"\n",
    "    },\n",
    "):\n",
    "    user_input = str(event)\n",
    "\n",
    "# Ask questions based on the stored input\n",
    "user_question = input(\"You: \")\n",
    "\n",
    "# Run the model with the stored input and the user's question\n",
    "for event in client.stream(\n",
    "    model_name,\n",
    "    input={\n",
    "        \"prompt\": f\"{user_input}\\n\\nUser Question: {user_question}\"\n",
    "    },\n",
    "):\n",
    "    print(str(event), end=\"\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
